\section{Design}
\label{sec:design}

\subsection{General framework}
In CRAQamo, each object is given a tag of strong consistency or weak consistency.  In all reads, we check the object's consistency level.  If it is a weak consistency object, then we immediately return the data.  If extra time is allowed, we can send a read request to other nodes, and return the newest version of the object, and overwrite the out of date data.  If it is a strong consistency object, we perform the standard algorithm of CRAQ.  In all writes, we check the object's consistency level.  If it is a write to a strongly consistent object, we send a write to the head and apply the normal CRAQ write pipeline. If it is a write to a weakly consistent object, we multicast the write to all servers.  If we have extra time, based on a user-defined value, $k$, we wait for $k$ successful responses to come back, at which point we notify the user. This will contribute by being able to provide a dual storage mechanism, we will be able to meet the needs of the user in terms of efficient consistency when needed, and less efficient consistency for increased throughput when needed.\\

We are planning on running similar evaluations as CRAQ to confirm that our additions do not slow it down in the strongly consistent case.  However, we will be doing additional tests with scalability at a global scale.  We are planning on running on multiple nodes, with enforced high latency between them, simulating data centers spread around the globe.  We think that we will be able to greatly outperform CRAQ in this situation, when we are operating with weakly consistent objects.\\

Our work plan initially begins with setting up CRAQ, as we think that a large portion of the difficulty will lie in the understanding of how to build and deploy it.  After getting that done, we are planning on attempting modifications to add data on each object about whether it is strongly or eventually consistent.  Next, we will at the breaking points between where we are strongly and eventually consistent, and will code up the eventual consistency.  Lastly, we will add the ability to change consistency with a special style of write, and do the extra work in user modifications we want to get to if we have extra time.\\

We are planning on dividing the labor evenly.  Aside from small investigations we are working on setting up and building the system, we are planning on doing the work together.  We will work as a group together on the code.  We do not want to formalize exactly what individual people will be doing, as we do not yet know the system well enough to divide up the labor completely.  Instead we will use tri-programming until we get a good basis for being able to then separate the tasks.  This will likely occur after we have figured how to build and deploy the system, and after we have learned how to add strong vs eventual consistency tags.  Tentatively, we will split it up by one group member handling reads, one group member handling writes, and one group member handling conflicts and failure resolutions.

\subsection{Interfaces}

\subsection{Strong consistency read and write and  failure recovery}
Our system is built upon CRAQ, thus it has the full capability of CRAQ. CRAQ naturally supports strongly consistent read and write. We keep the strongly consistent storage interfaces in CRAQ and leave an additional flag for the user to determine whether to apply strongly consistent operations. We also adopt the failure recovery mechanism in CRAQ.

\subsection{Broadcasting data}
In CRAQ, data is only passed between neighboring nodes, which causes the write operation inefficient because each write operation needs to go through all the node although strong consistency is guaranteed. In CRAQamo, when strong consistency is not a must, we allow direct communication between any two nodes. Specifically, we have an internal method called ``eventual consistency propagate" to help pass information globally among all the nods. We show the necessity of having this internal helper method in the following sections.

\subsection{Comparing versions}
Like CRAQ, each node in the chain also store multiple versions of objects in CRAQamo. CRAQ uses a monotonically-increasing number as the version number, which is local to the node. Synchronization is not a problem to CRAQ since as mentioned previously, data is only passed between neighboring nodes. However, such a simple representation for the version can handle the necessity of global broadcasting well. Instead, CRAQamo uses global timestamps to represent versions. We would like to emphasize that replacing monotonically-increasing version numbers with timestamps do not affect the original functionalities of CRAQ, thus strongly consistent operations are still supported by CRAQ and we do not discuss them in detail here.

For eventually consistent operations, we need to remotely compare versions of objects between nodes. Therefore it is necessary for each node to fetch the newest timestamps on other nodes and return boolean values indicating whether the current node keeps the newest version of the object of interest. Specifically, a node signals other nodes to return their current timestamps. Since the timestamps are global, comparison and be easily done by comparing the actual values of timestamps (larger value means newer and vice versa). Such version comparing method also severs as an internal helper method we detail the usages in the following sections. 

\subsection{Eventually consistent read}
Eventual consistency allows read operations to return newly written objects before they are fully synchronized among all nodes. This is the key to improve the read throughput comparing with CRAQ because CRAQ is constrained by the strong consistency capability, thus only committed objects can be returned in CRAQ. In the eventual consistency mode of CRAQamo, the newly written object can be directly return to the requester as long as the timestamp of the object is sufficiently new. Note that it is not strictly required that the timestamp of the object on the current node is the newest. We apply a Dynamo style parameter $R$ so that the user can determine the level of eventual consistency.

When an eventually consistent read operation is performed on a node. The node first compares the timestamp of the requested object with the timestamps of the object on all other nodes. To speed up the process, the comparisons are done in parallel. Once there are $R$ other nodes agree that the current node has the newer version of object, the current node directly returns the object to the user. If any of the $R$ nodes disagrees that the current node has the newer version of object, ``eventual consistency propagate" is executed so that the newer object can be passed from the node which is holding the new version of object to the current node. By varying $R$, the level of eventual consistency can be flexibly changed. For example, lowering $R$ lowers the probability that the slow ``eventual consistency propagate" method is called so that the overall read throughput can be increased.

\subsection{Eventually consistent write}
A critical reason that chain replication storage systems are generally slow in write operations is that the write operation needs to go through all the nodes in the chain. While this guarantees strong consistency, the latency is also large when nodes are placed spatially distant to each other. However, if strong consistency is not a must, one can choose to write new data to any node in the chain instead of writing to the head node. In CRAQamo, we allow the user to write to any node and safely check whether there are other writes performed on other nodes when the write operation is being performed. Note that write operations happening almost concurrently on multiple nodes rarely happens, so the synchronization can take slightly longer time and it will not critically affect the overall write throughput.

When an eventually consistent write operation is performed on a node. The node first takes the input object, writes it locally and attaches a timestamp to the updated object. There is a possibility that when it writes the object, another write operation of the same object is performed on another node, thus the written object is no longer the newest even if the current node finishes writing. To make sure that the newly written data is the newest, a safety check is done when the write finishes. The current node compares the timestamp of the newly written object with the timestamps of the object on all other nodes. Once there are $W$ other nodes agree that the current node has the newer version of object, the current node signals the user that the write is successful and uses ``eventual consistency propagate" to broadcast the newly written object to all other nodes. If any of the $W$ nodes disagrees that the current node has the newer version of object, ``eventual consistency propagate" is also executed so that the newer object can be synchronized among all nodes. After synchronization, the current node signals the user that the write is successful. 


\subsection{Implementation}
Our prototype implementation of CRAQamo was implemented in approximately 3700 lines of C++,
built upon the a prototype implementation of CRAQ implemented in approximately 3000 lines of C++.
Like CRAQ, CRAQamo uses the Tame \cite{tame} extension to the SFS asynchronous I/O and RPC libraries \cite{sfs} and all network functionality between CRAQamo RPC nodes is exposed via Sun RPC interfaces.
Unlike CRAQ, CRAQamo also uses the built in C++ multithreading library.
To implement the broadcast reads and writes for consistency checks, CRAQamo needs to spin up multiple threads, and wait until $w$ of them complete their calls before returning.
Even after returning, however, CRAQamo needs to continue the consistency checks for the nodes that haven't yet completed.
This proved too difficult of a pattern to express in Tame, so we instead built our own manner of multithreading for these checks.
However, for all other asynchronous calls, we used Tame to decrease code complexity.

Our integration with ZooKeeper was provided with by the expansion of CRAQ, and was integrated in the same way, with CRAQamo nodes able to query the same ZooKeeper files as CRAQ nodes.
Furthermore, expansion of CRAQ gave us free chain node functionality and the ability to handle membership changes dynamically.
Because data is stored in the same manner as CRAQ, and because no node has special knowledge in CRAQamo, CRAQ's algorithm for membership changes proves sufficient for CRAQamo.
